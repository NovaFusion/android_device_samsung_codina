From 880f85f8487e36c9909c4f3e042fd40388338f5d Mon Sep 17 00:00:00 2001
From: Marcin Chojnacki <marcinch7@gmail.com>
Date: Thu, 23 Jan 2014 17:09:35 +0100
Subject: [PATCH] legacy bionic dla multimediow

Change-Id: Ie7e25c54a4d3c4d0fdaa540732b841fa667caf2d
---
 libc/arch-arm/bionic/memcmp16.S              |  28 ++--
 libc/arch-arm/bionic/memcpy.S                |  14 +-
 libc/arch-arm/bionic/strcmp.S                |   8 +-
 libc/arch-arm/generic/bionic/memcpy.S        |  14 +-
 libc/arch-arm/generic/bionic/strcmp.S        |   8 +-
 libc/arch-arm/generic/bionic/strcpy.S        |   4 +-
 libc/arch-arm/generic/bionic/strlen.c        |   2 +
 libc/arch-arm/include/machine/cpu-features.h | 128 ++++++++++++++-
 libc/arch-arm/include/machine/endian.h       |  14 +-
 libc/bionic/pthread.c                        |  25 ++-
 libc/private/bionic_atomic_arm.h             | 222 +++++++++++++++++++++------
 libc/private/bionic_atomic_gcc_builtin.h     |  49 +++---
 libc/private/bionic_atomic_inline.h          |   7 +-
 libc/private/bionic_atomic_mips.h            | 106 ++++++++-----
 libc/private/bionic_atomic_x86.h             |  67 +++++---
 15 files changed, 522 insertions(+), 174 deletions(-)

diff --git a/libc/arch-arm/bionic/memcmp16.S b/libc/arch-arm/bionic/memcmp16.S
index 8239441..99c9b88 100644
--- a/libc/arch-arm/bionic/memcmp16.S
+++ b/libc/arch-arm/bionic/memcmp16.S
@@ -32,15 +32,15 @@
 /*
  * Optimized memcmp16() for ARM9.
  * This would not be optimal on XScale or ARM11, where more prefetching
- * and use of pld will be needed.
+ * and use of PLD will be needed.
  * The 2 major optimzations here are
  * (1) The main loop compares 16 bytes at a time
  * (2) The loads are scheduled in a way they won't stall
  */
 
 ENTRY(__memcmp16)
-        pld         [r0, #0]
-        pld         [r1, #0]
+        PLD         (r0, #0)
+        PLD         (r1, #0)
 
         /* take of the case where length is nul or the buffers are the same */
         cmp         r0, r1
@@ -62,13 +62,13 @@ ENTRY(__memcmp16)
         bpl         0f
 
         /* small blocks (less then 12 words) */
-        pld         [r0, #32]
-        pld         [r1, #32]
+        PLD         (r0, #32)
+        PLD         (r1, #32)
 
 1:      ldrh        r0, [r3], #2
         ldrh        ip, [r1], #2
         subs        r0, r0, ip
-        bxne        lr
+        bxne        lr        
         subs        r2, r2, #1
         bne         1b
         bx          lr
@@ -81,7 +81,7 @@ ENTRY(__memcmp16)
         /* align first pointer to word boundary */
         tst         r3, #2
         beq         0f
-
+        
         ldrh        r0, [r3], #2
         ldrh        ip, [r1], #2
         sub         r2, r2, #1
@@ -109,10 +109,10 @@ ENTRY(__memcmp16)
         ldr         ip, [r1]
         subs        r2, r2, #(16 + 2)
         bmi         1f
-
+        
 0:
-        pld         [r3, #64]
-        pld         [r1, #64]
+        PLD         (r3, #64)
+        PLD         (r1, #64)
         ldr         r0, [r3], #4
         ldr         lr, [r1, #4]!
         eors        r0, r0, ip
@@ -137,14 +137,14 @@ ENTRY(__memcmp16)
         ldreq       r0, [r3], #4
         ldreq       ip, [r1, #4]!
         eoreqs      r0, r0, lr
-        bne         2f
+        bne         2f        
         subs        r2, r2, #16
         bhs         0b
 
         /* do we have at least 2 words left? */
 1:      adds        r2, r2, #(16 - 2 + 2)
         bmi         4f
-
+        
         /* finish off 2 words at a time */
 3:      ldr         r0, [r3], #4
         ldr         ip, [r1], #4
@@ -193,8 +193,8 @@ ENTRY(__memcmp16)
         sub         r2, r2, #8
 
 6:
-        pld         [r3, #64]
-        pld         [r1, #64]
+        PLD         (r3, #64)
+        PLD         (r1, #64)
         mov         ip, lr, lsr #16
         ldr         lr, [r1], #4
         ldr         r0, [r3], #4
diff --git a/libc/arch-arm/bionic/memcpy.S b/libc/arch-arm/bionic/memcpy.S
index f25b3e3..0dc86d5 100644
--- a/libc/arch-arm/bionic/memcpy.S
+++ b/libc/arch-arm/bionic/memcpy.S
@@ -352,9 +352,9 @@ ENTRY(memcpy)
 
         // preload the destination because we'll align it to a cache line
         // with small writes. Also start the source "pump".
-        pld         [r0, #0]
-        pld         [r1, #0]
-        pld         [r1, #32]
+        PLD         (r0, #0)
+        PLD         (r1, #0)
+        PLD         (r1, #32)
 
 		/* it simplifies things to take care of len<4 early */
 		cmp			r2, #4
@@ -442,7 +442,7 @@ cached_aligned32:
         add         r12, r12, #64
 
 1:      ldmia       r1!, { r4-r11 }
-        pld         [r12, #64]
+        PLD         (r12, #64)
         subs        r2, r2, #32
 
         // NOTE: if r12 is more than 64 ahead of r1, the following ldrhi
@@ -563,7 +563,7 @@ loop16:
         ldr         r12, [r1], #4
 1:      mov         r4, r12
 		ldmia		r1!, {   r5,r6,r7,  r8,r9,r10,r11}
-        pld         [r1, #64]
+        PLD         (r1, #64)
         subs        r2, r2, #32
         ldrhs       r12, [r1], #4
 		orr			r3, r3, r4,		lsl #16
@@ -590,7 +590,7 @@ loop8:
         ldr         r12, [r1], #4
 1:      mov         r4, r12
 		ldmia		r1!, {   r5,r6,r7,  r8,r9,r10,r11}
-        pld         [r1, #64]
+        PLD         (r1, #64)
 		subs		r2, r2, #32
         ldrhs       r12, [r1], #4
 		orr			r3, r3, r4,		lsl #24
@@ -617,7 +617,7 @@ loop24:
         ldr         r12, [r1], #4
 1:      mov         r4, r12
 		ldmia		r1!, {   r5,r6,r7,  r8,r9,r10,r11}
-        pld         [r1, #64]
+        PLD         (r1, #64)
 		subs		r2, r2, #32
         ldrhs       r12, [r1], #4
 		orr			r3, r3, r4,		lsl #8
diff --git a/libc/arch-arm/bionic/strcmp.S b/libc/arch-arm/bionic/strcmp.S
index 42d41d1..764a531 100644
--- a/libc/arch-arm/bionic/strcmp.S
+++ b/libc/arch-arm/bionic/strcmp.S
@@ -52,8 +52,8 @@
 #define magic2(REG) REG, lsl #7
 
 ENTRY(strcmp)
-	pld	[r0, #0]
-	pld	[r1, #0]
+	PLD(r0, #0)
+	PLD(r1, #0)
 	eor	r2, r0, r1
 	tst	r2, #3
 
@@ -88,8 +88,8 @@ ENTRY(strcmp)
 	orr	r4, r4, r4, lsl #16
 	.p2align	2
 4:
-	pld	[r0, #8]
-	pld	[r1, #8]
+	PLD(r0, #8)
+	PLD(r1, #8)
 	sub	r2, ip, magic1(r4)
 	cmp	ip, r3
 	itttt	eq
diff --git a/libc/arch-arm/generic/bionic/memcpy.S b/libc/arch-arm/generic/bionic/memcpy.S
index f08817b..24373d8 100644
--- a/libc/arch-arm/generic/bionic/memcpy.S
+++ b/libc/arch-arm/generic/bionic/memcpy.S
@@ -57,9 +57,9 @@ ENTRY(memcpy)
 
         // preload the destination because we'll align it to a cache line
         // with small writes. Also start the source "pump".
-        pld         [r0, #0]
-        pld         [r1, #0]
-        pld         [r1, #32]
+        PLD         (r0, #0)
+        PLD         (r1, #0)
+        PLD         (r1, #32)
 
         /* it simplifies things to take care of len<4 early */
         cmp         r2, #4
@@ -147,7 +147,7 @@ cached_aligned32:
         add         r12, r12, #64
 
 1:      ldmia       r1!, { r4-r11 }
-        pld         [r12, #64]
+        PLD         (r12, #64)
         subs        r2, r2, #32
 
         // NOTE: if r12 is more than 64 ahead of r1, the following ldrhi
@@ -268,7 +268,7 @@ loop16:
         ldr         r12, [r1], #4
 1:      mov         r4, r12
         ldmia       r1!, {   r5,r6,r7,  r8,r9,r10,r11}
-        pld         [r1, #64]
+        PLD         (r1, #64)
         subs        r2, r2, #32
         ldrhs       r12, [r1], #4
         orr         r3, r3, r4,     lsl #16
@@ -295,7 +295,7 @@ loop8:
         ldr         r12, [r1], #4
 1:      mov         r4, r12
         ldmia       r1!, {   r5,r6,r7,  r8,r9,r10,r11}
-        pld         [r1, #64]
+        PLD         (r1, #64)
         subs        r2, r2, #32
         ldrhs       r12, [r1], #4
         orr         r3, r3, r4,     lsl #24
@@ -322,7 +322,7 @@ loop24:
         ldr         r12, [r1], #4
 1:      mov         r4, r12
         ldmia       r1!, {   r5,r6,r7,  r8,r9,r10,r11}
-        pld         [r1, #64]
+        PLD         (r1, #64)
         subs        r2, r2, #32
         ldrhs       r12, [r1], #4
         orr         r3, r3, r4,     lsl #8
diff --git a/libc/arch-arm/generic/bionic/strcmp.S b/libc/arch-arm/generic/bionic/strcmp.S
index 42d41d1..764a531 100644
--- a/libc/arch-arm/generic/bionic/strcmp.S
+++ b/libc/arch-arm/generic/bionic/strcmp.S
@@ -52,8 +52,8 @@
 #define magic2(REG) REG, lsl #7
 
 ENTRY(strcmp)
-	pld	[r0, #0]
-	pld	[r1, #0]
+	PLD(r0, #0)
+	PLD(r1, #0)
 	eor	r2, r0, r1
 	tst	r2, #3
 
@@ -88,8 +88,8 @@ ENTRY(strcmp)
 	orr	r4, r4, r4, lsl #16
 	.p2align	2
 4:
-	pld	[r0, #8]
-	pld	[r1, #8]
+	PLD(r0, #8)
+	PLD(r1, #8)
 	sub	r2, ip, magic1(r4)
 	cmp	ip, r3
 	itttt	eq
diff --git a/libc/arch-arm/generic/bionic/strcpy.S b/libc/arch-arm/generic/bionic/strcpy.S
index cc997f4..21dafda 100644
--- a/libc/arch-arm/generic/bionic/strcpy.S
+++ b/libc/arch-arm/generic/bionic/strcpy.S
@@ -33,7 +33,7 @@
 #include <machine/asm.h>
 
 ENTRY(strcpy)
-	pld	[r1, #0]
+	PLD(r1, #0)
 	eor	r2, r0, r1
 	mov	ip, r0
 	tst	r2, #3
@@ -62,7 +62,7 @@ ENTRY(strcpy)
 	  load stalls.  */
 	.p2align 2
 2:
-	pld	[r1, #8]
+	PLD(r1, #8)
 	ldr	r4, [r1], #4
 	sub	r2, r3, r5
 	bics	r2, r2, r3
diff --git a/libc/arch-arm/generic/bionic/strlen.c b/libc/arch-arm/generic/bionic/strlen.c
index 811e1e0..824cf78 100644
--- a/libc/arch-arm/generic/bionic/strlen.c
+++ b/libc/arch-arm/generic/bionic/strlen.c
@@ -63,7 +63,9 @@ size_t strlen(const char *s)
         "ldr     %[v], [%[s]], #4           \n"
         "sub     %[l], %[l], %[s]           \n"
         "0:                                 \n"
+#if __ARM_HAVE_PLD
         "pld     [%[s], #64]                \n"
+#endif
         "sub     %[t], %[v], %[mask], lsr #7\n"
         "and     %[t], %[t], %[mask]        \n"
         "bics    %[t], %[t], %[v]           \n"
diff --git a/libc/arch-arm/include/machine/cpu-features.h b/libc/arch-arm/include/machine/cpu-features.h
index fc5a8fd..80d3fda 100644
--- a/libc/arch-arm/include/machine/cpu-features.h
+++ b/libc/arch-arm/include/machine/cpu-features.h
@@ -34,29 +34,133 @@
  *
  * This is done to abstract us from the various ARM Architecture
  * quirks and alphabet soup.
+ *
+ * IMPORTANT: We have no intention to support anything below an ARMv4T !
  */
 
 /* __ARM_ARCH__ is a number corresponding to the ARM revision
- * we're going to support. Our toolchain doesn't define __ARM_ARCH__
+ * we're going to support
+ *
+ * it looks like our toolchain doesn't define __ARM_ARCH__
  * so try to guess it.
+ *
+ *
+ *
  */
 #ifndef __ARM_ARCH__
+
 #  if defined __ARM_ARCH_7__   || defined __ARM_ARCH_7A__ || \
-        defined __ARM_ARCH_7R__  || defined __ARM_ARCH_7M__
+      defined __ARM_ARCH_7R__  || defined __ARM_ARCH_7M__
+
 #    define __ARM_ARCH__ 7
+
 #  elif defined __ARM_ARCH_6__   || defined __ARM_ARCH_6J__ || \
-        defined __ARM_ARCH_6K__  || defined __ARM_ARCH_6Z__ || \
-        defined __ARM_ARCH_6KZ__ || defined __ARM_ARCH_6T2__
+      defined __ARM_ARCH_6K__  || defined __ARM_ARCH_6Z__ || \
+      defined __ARM_ARCH_6KZ__ || defined __ARM_ARCH_6T2__
+#
 #    define __ARM_ARCH__ 6
+#
+#  elif defined __ARM_ARCH_5__ || defined __ARM_ARCH_5T__ || \
+        defined __ARM_ARCH_5TE__ || defined __ARM_ARCH_5TEJ__
+#
+#    define __ARM_ARCH__ 5
+#
+#  elif defined __ARM_ARCH_4T__
+#
+#    define __ARM_ARCH__ 4
+#
+#  elif defined __ARM_ARCH_4__
+#    error ARMv4 is not supported, please use ARMv4T at a minimum
 #  else
 #    error Unknown or unsupported ARM architecture
 #  endif
 #endif
 
+/* experimental feature used to check that our ARMv4 workarounds
+ * work correctly without a real ARMv4 machine */
+#ifdef BIONIC_EXPERIMENTAL_FORCE_ARMV4
+#  undef  __ARM_ARCH__
+#  define __ARM_ARCH__  4
+#endif
+
+/* define __ARM_HAVE_5TE if we have the ARMv5TE instructions */
+#if __ARM_ARCH__ > 5
+#  define  __ARM_HAVE_5TE  1
+#elif __ARM_ARCH__ == 5
+#  if defined __ARM_ARCH_5TE__ || defined __ARM_ARCH_5TEJ__
+#    define __ARM_HAVE_5TE  1
+#  endif
+#endif
+
+/* instructions introduced in ARMv5 */
+#if __ARM_ARCH__ >= 5
+#  define  __ARM_HAVE_BLX  1
+#  define  __ARM_HAVE_CLZ  1
+#  define  __ARM_HAVE_LDC2 1
+#  define  __ARM_HAVE_MCR2 1
+#  define  __ARM_HAVE_MRC2 1
+#  define  __ARM_HAVE_STC2 1
+#endif
+
+/* ARMv5TE introduces a few instructions */
+#if __ARM_HAVE_5TE
+#  define  __ARM_HAVE_PLD   1
+#  define  __ARM_HAVE_MCRR  1
+#  define  __ARM_HAVE_MRRC  1
+#endif
+
 /* define __ARM_HAVE_HALFWORD_MULTIPLY when half-word multiply instructions
  * this means variants of: smul, smulw, smla, smlaw, smlal
  */
-#define  __ARM_HAVE_HALFWORD_MULTIPLY  1
+#if __ARM_HAVE_5TE
+#  define  __ARM_HAVE_HALFWORD_MULTIPLY  1
+#endif
+
+/* define __ARM_HAVE_PAIR_LOAD_STORE when 64-bit memory loads and stored
+ * into/from a pair of 32-bit registers is supported throuhg 'ldrd' and 'strd'
+ */
+#if __ARM_HAVE_5TE
+#  define  __ARM_HAVE_PAIR_LOAD_STORE 1
+#endif
+
+/* define __ARM_HAVE_SATURATED_ARITHMETIC is you have the saturated integer
+ * arithmetic instructions: qdd, qdadd, qsub, qdsub
+ */
+#if __ARM_HAVE_5TE
+#  define  __ARM_HAVE_SATURATED_ARITHMETIC 1
+#endif
+
+/* define __ARM_HAVE_PC_INTERWORK when a direct assignment to the
+ * pc register will switch into thumb/ARM mode depending on bit 0
+ * of the new instruction address. Before ARMv5, this was not the
+ * case, and you have to write:
+ *
+ *     mov  r0, [<some address>]
+ *     bx   r0
+ *
+ * instead of:
+ *
+ *     ldr  pc, [<some address>]
+ *
+ * note that this affects any instruction that explicitly changes the
+ * value of the pc register, including ldm { ...,pc } or 'add pc, #offset'
+ */
+#if __ARM_ARCH__ >= 5
+#  define __ARM_HAVE_PC_INTERWORK
+#endif
+
+/* define __ARM_HAVE_LDREX_STREX for ARMv6 and ARMv7 architecture to be
+ * used in replacement of deprecated swp instruction
+ */
+#if __ARM_ARCH__ >= 6
+#  define __ARM_HAVE_LDREX_STREX
+#endif
+
+/* define __ARM_HAVE_DMB for ARMv7 architecture
+ */
+#if __ARM_ARCH__ >= 7
+#  define __ARM_HAVE_DMB
+#endif
 
 /* define __ARM_HAVE_LDREXD for ARMv7 architecture
  * (also present in ARMv6K, but not implemented in ARMv7-M, neither of which
@@ -80,4 +184,18 @@
 #  define __ARM_HAVE_NEON
 #endif
 
+/* Assembly-only macros */
+#ifdef __ASSEMBLY__
+
+/* define a handy PLD(address) macro since the cache preload
+ * is an optional opcode
+ */
+#if __ARM_HAVE_PLD
+#  define  PLD(reg,offset)    pld    [reg, offset]
+#else
+#  define  PLD(reg,offset)    /* nothing */
+#endif
+
+#endif /* ! __ASSEMBLY__ */
+
 #endif /* _ARM_MACHINE_CPU_FEATURES_H */
diff --git a/libc/arch-arm/include/machine/endian.h b/libc/arch-arm/include/machine/endian.h
index 8d9723d..7cba3b9 100644
--- a/libc/arch-arm/include/machine/endian.h
+++ b/libc/arch-arm/include/machine/endian.h
@@ -33,6 +33,15 @@
 
 #ifdef __GNUC__
 
+/*
+ * REV and REV16 weren't available on ARM5 or ARM4.
+ * We don't include <machine/cpu-features.h> because it pollutes the
+ * namespace with macros like PLD.
+ */
+#if !defined __ARM_ARCH_5__ && !defined __ARM_ARCH_5T__ && \
+    !defined __ARM_ARCH_5TE__ && !defined __ARM_ARCH_5TEJ__ && \
+    !defined __ARM_ARCH_4T__ && !defined __ARM_ARCH_4__
+
 /* According to RealView Assembler User's Guide, REV and REV16 are available
  * in Thumb code and 16-bit instructions when used in Thumb-2 code.
  *
@@ -46,13 +55,13 @@
  */
 #define __swap16md(x) ({                                        \
     register u_int16_t _x = (x);                                \
-    __asm__ __volatile__("rev16 %0, %0" : "+l" (_x));           \
+    __asm volatile ("rev16 %0, %0" : "+l" (_x));                \
     _x;                                                         \
 })
 
 #define __swap32md(x) ({                                        \
     register u_int32_t _x = (x);                                \
-    __asm__ __volatile__("rev %0, %0" : "+l" (_x));             \
+    __asm volatile ("rev %0, %0" : "+l" (_x));                  \
     _x;                                                         \
 })
 
@@ -65,6 +74,7 @@
 /* Tell sys/endian.h we have MD variants of the swap macros.  */
 #define MD_SWAP
 
+#endif  /* __ARM_ARCH__ */
 #endif  /* __GNUC__ */
 
 #if defined(__ARMEB__)
diff --git a/libc/bionic/pthread.c b/libc/bionic/pthread.c
index 764e01d..92e2c27 100644
--- a/libc/bionic/pthread.c
+++ b/libc/bionic/pthread.c
@@ -391,16 +391,21 @@ int pthread_mutexattr_getpshared(pthread_mutexattr_t *attr, int *pshared)
     return 0;
 }
 
-int pthread_mutex_init(pthread_mutex_t* mutex, const pthread_mutexattr_t* attr) {
+int pthread_mutex_init(pthread_mutex_t *mutex,
+                       const pthread_mutexattr_t *attr)
+{
+    int value = 0;
+
+    if (mutex == NULL)
+        return EINVAL;
+
     if (__predict_true(attr == NULL)) {
         mutex->value = MUTEX_TYPE_BITS_NORMAL;
         return 0;
     }
 
-    int value = 0;
-    if ((*attr & MUTEXATTR_SHARED_MASK) != 0) {
+    if ((*attr & MUTEXATTR_SHARED_MASK) != 0)
         value |= MUTEX_SHARED_MASK;
-    }
 
     switch (*attr & MUTEXATTR_TYPE_MASK) {
     case PTHREAD_MUTEX_NORMAL:
@@ -577,6 +582,9 @@ int pthread_mutex_lock_impl(pthread_mutex_t *mutex)
 {
     int mvalue, mtype, tid, shared;
 
+    if (__predict_false(mutex == NULL))
+        return EINVAL;
+
     mvalue = mutex->value;
     mtype = (mvalue & MUTEX_TYPE_MASK);
     shared = (mvalue & MUTEX_SHARED_MASK);
@@ -668,6 +676,9 @@ int pthread_mutex_unlock_impl(pthread_mutex_t *mutex)
 {
     int mvalue, mtype, tid, shared;
 
+    if (__predict_false(mutex == NULL))
+        return EINVAL;
+
     mvalue = mutex->value;
     mtype  = (mvalue & MUTEX_TYPE_MASK);
     shared = (mvalue & MUTEX_SHARED_MASK);
@@ -732,6 +743,9 @@ int pthread_mutex_trylock_impl(pthread_mutex_t *mutex)
 {
     int mvalue, mtype, tid, shared;
 
+    if (__predict_false(mutex == NULL))
+        return EINVAL;
+
     mvalue = mutex->value;
     mtype  = (mvalue & MUTEX_TYPE_MASK);
     shared = (mvalue & MUTEX_SHARED_MASK);
@@ -827,6 +841,9 @@ int pthread_mutex_lock_timeout_np_impl(pthread_mutex_t *mutex, unsigned msecs)
     /* compute absolute expiration time */
     __timespec_to_relative_msec(&abstime, msecs, clock);
 
+    if (__predict_false(mutex == NULL))
+        return EINVAL;
+
     mvalue = mutex->value;
     mtype  = (mvalue & MUTEX_TYPE_MASK);
     shared = (mvalue & MUTEX_SHARED_MASK);
diff --git a/libc/private/bionic_atomic_arm.h b/libc/private/bionic_atomic_arm.h
index 7f31d53..3bb639e 100644
--- a/libc/private/bionic_atomic_arm.h
+++ b/libc/private/bionic_atomic_arm.h
@@ -16,64 +16,198 @@
 #ifndef BIONIC_ATOMIC_ARM_H
 #define BIONIC_ATOMIC_ARM_H
 
-__ATOMIC_INLINE__ void __bionic_memory_barrier() {
-#if defined(ANDROID_SMP) && ANDROID_SMP == 1
-  __asm__ __volatile__ ( "dmb ish" : : : "memory" );
+#include <machine/cpu-features.h>
+
+/* Some of the harware instructions used below are not available in Thumb-1
+ * mode (they are if you build in ARM or Thumb-2 mode though). To solve this
+ * problem, we're going to use the same technique than libatomics_ops,
+ * which is to temporarily switch to ARM, do the operation, then switch
+ * back to Thumb-1.
+ *
+ * This results in two 'bx' jumps, just like a normal function call, but
+ * everything is kept inlined, avoids loading or computing the function's
+ * address, and prevents a little I-cache trashing too.
+ *
+ * However, it is highly recommended to avoid compiling any C library source
+ * file that use these functions in Thumb-1 mode.
+ *
+ * Define three helper macros to implement this:
+ */
+#if defined(__thumb__) && !defined(__thumb2__)
+#  define  __ATOMIC_SWITCH_TO_ARM \
+            "adr r3, 5f\n" \
+            "bx  r3\n" \
+            ".align\n" \
+            ".arm\n" \
+        "5:\n"
+/* note: the leading \n below is intentional */
+#  define __ATOMIC_SWITCH_TO_THUMB \
+            "\n" \
+            "adr r3, 6f\n" \
+            "bx  r3\n" \
+            ".thumb" \
+        "6:\n"
+
+#  define __ATOMIC_CLOBBERS   "r3"  /* list of clobbered registers */
+
+/* Warn the user that ARM mode should really be preferred! */
+#  warning Rebuilding this source file in ARM mode is highly recommended for performance!!
+
 #else
-  /* A simple compiler barrier. */
-  __asm__ __volatile__ ( "" : : : "memory" );
+#  define  __ATOMIC_SWITCH_TO_ARM   /* nothing */
+#  define  __ATOMIC_SWITCH_TO_THUMB /* nothing */
+#  define  __ATOMIC_CLOBBERS        /* nothing */
 #endif
+
+
+/* Define a full memory barrier, this is only needed if we build the
+ * platform for a multi-core device. For the record, using a 'dmb'
+ * instruction on a Nexus One device can take up to 180 ns even if
+ * it is completely un-necessary on this device.
+ *
+ * NOTE: This is where the platform and NDK headers atomic headers are
+ *        going to diverge. With the NDK, we don't know if the generated
+ *        code is going to run on a single or multi-core device, so we
+ *        need to be cautious.
+ *
+ *        I.e. on single-core devices, the helper immediately returns,
+ *        on multi-core devices, it uses "dmb" or any other means to
+ *        perform a full-memory barrier.
+ *
+ * There are three cases to consider for the platform:
+ *
+ *    - multi-core ARMv7-A       => use the 'dmb' hardware instruction
+ *    - multi-core ARMv6         => use the coprocessor
+ *    - single core ARMv6+       => do not use any hardware barrier
+ */
+#if defined(ANDROID_SMP) && ANDROID_SMP == 1
+
+/* Sanity check, multi-core is only supported starting from ARMv6 */
+#  if __ARM_ARCH__ < 6
+#    error ANDROID_SMP should not be set to 1 for an ARM architecture less than 6
+#  endif
+
+#  ifdef __ARM_HAVE_DMB
+/* For ARMv7-A, we can use the 'dmb' instruction directly */
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier(void)
+{
+    /* Note: we always build in ARM or Thumb-2 on ARMv7-A, so don't
+     * bother with __ATOMIC_SWITCH_TO_ARM */
+    __asm__ __volatile__ ( "dmb" : : : "memory" );
+}
+#  else /* !__ARM_HAVE_DMB */
+/* Otherwise, i.e. for multi-core ARMv6, we need to use the coprocessor,
+ * which requires the use of a general-purpose register, which is slightly
+ * less efficient.
+ */
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier(void)
+{
+    __asm__ __volatile__ (
+        __SWITCH_TO_ARM
+        "mcr p15, 0, %0, c7, c10, 5"
+        __SWITCH_TO_THUMB
+        : : "r" (0) : __ATOMIC_CLOBBERS "memory");
 }
+#  endif /* !__ARM_HAVE_DMB */
+#else /* !ANDROID_SMP */
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier(void)
+{
+    /* A simple compiler barrier */
+    __asm__ __volatile__ ( "" : : : "memory" );
+}
+#endif /* !ANDROID_SMP */
+
+#ifndef __ARM_HAVE_LDREX_STREX
+#error Only ARM devices which have LDREX / STREX are supported
+#endif
 
 /* Compare-and-swap, without any explicit barriers. Note that this functions
  * returns 0 on success, and 1 on failure. The opposite convention is typically
  * used on other platforms.
  */
-__ATOMIC_INLINE__ int __bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr) {
-  int32_t prev, status;
-  do {
-    __asm__ __volatile__ (
-          "ldrex %0, [%3]\n"
-          "mov %1, #0\n"
-          "teq %0, %4\n"
+__ATOMIC_INLINE__ int
+__bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr)
+{
+    int32_t prev, status;
+    do {
+        __asm__ __volatile__ (
+            __ATOMIC_SWITCH_TO_ARM
+            "ldrex %0, [%3]\n"
+            "mov %1, #0\n"
+            "teq %0, %4\n"
 #ifdef __thumb2__
-          "it eq\n"
+            "it eq\n"
 #endif
-          "strexeq %1, %5, [%3]"
-          : "=&r" (prev), "=&r" (status), "+m"(*ptr)
-          : "r" (ptr), "Ir" (old_value), "r" (new_value)
-          : "cc");
-  } while (__builtin_expect(status != 0, 0));
-  return prev != old_value;
+            "strexeq %1, %5, [%3]"
+            __ATOMIC_SWITCH_TO_THUMB
+            : "=&r" (prev), "=&r" (status), "+m"(*ptr)
+            : "r" (ptr), "Ir" (old_value), "r" (new_value)
+            : __ATOMIC_CLOBBERS "cc");
+    } while (__builtin_expect(status != 0, 0));
+    return prev != old_value;
 }
 
-/* Swap, without any explicit barriers. */
-__ATOMIC_INLINE__ int32_t __bionic_swap(int32_t new_value, volatile int32_t* ptr) {
-  int32_t prev, status;
-  do {
-    __asm__ __volatile__ (
-          "ldrex %0, [%3]\n"
-          "strex %1, %4, [%3]"
-          : "=&r" (prev), "=&r" (status), "+m" (*ptr)
-          : "r" (ptr), "r" (new_value)
-          : "cc");
-  } while (__builtin_expect(status != 0, 0));
-  return prev;
+/* Swap operation, without any explicit barriers. */
+__ATOMIC_INLINE__ int32_t
+__bionic_swap(int32_t new_value, volatile int32_t* ptr)
+{
+    int32_t prev, status;
+    do {
+        __asm__ __volatile__ (
+            __ATOMIC_SWITCH_TO_ARM
+            "ldrex %0, [%3]\n"
+            "strex %1, %4, [%3]"
+            __ATOMIC_SWITCH_TO_THUMB
+            : "=&r" (prev), "=&r" (status), "+m" (*ptr)
+            : "r" (ptr), "r" (new_value)
+            : __ATOMIC_CLOBBERS "cc");
+    } while (__builtin_expect(status != 0, 0));
+    return prev;
 }
 
-/* Atomic decrement, without explicit barriers. */
-__ATOMIC_INLINE__ int32_t __bionic_atomic_dec(volatile int32_t* ptr) {
-  int32_t prev, tmp, status;
-  do {
-    __asm__ __volatile__ (
-          "ldrex %0, [%4]\n"
-          "sub %1, %0, #1\n"
-          "strex %2, %1, [%4]"
-          : "=&r" (prev), "=&r" (tmp), "=&r" (status), "+m"(*ptr)
-          : "r" (ptr)
-          : "cc");
-  } while (__builtin_expect(status != 0, 0));
-  return prev;
+/* Atomic increment - without any barriers
+ * This returns the old value
+ */
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_inc(volatile int32_t* ptr)
+{
+    int32_t prev, tmp, status;
+    do {
+        __asm__ __volatile__ (
+            __ATOMIC_SWITCH_TO_ARM
+            "ldrex %0, [%4]\n"
+            "add %1, %0, #1\n"
+            "strex %2, %1, [%4]"
+            __ATOMIC_SWITCH_TO_THUMB
+            : "=&r" (prev), "=&r" (tmp), "=&r" (status), "+m"(*ptr)
+            : "r" (ptr)
+            : __ATOMIC_CLOBBERS "cc");
+    } while (__builtin_expect(status != 0, 0));
+    return prev;
+}
+
+/* Atomic decrement - without any barriers
+ * This returns the old value.
+ */
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_dec(volatile int32_t* ptr)
+{
+    int32_t prev, tmp, status;
+    do {
+        __asm__ __volatile__ (
+            __ATOMIC_SWITCH_TO_ARM
+            "ldrex %0, [%4]\n"
+            "sub %1, %0, #1\n"
+            "strex %2, %1, [%4]"
+            __ATOMIC_SWITCH_TO_THUMB
+            : "=&r" (prev), "=&r" (tmp), "=&r" (status), "+m"(*ptr)
+            : "r" (ptr)
+            : __ATOMIC_CLOBBERS "cc");
+    } while (__builtin_expect(status != 0, 0));
+    return prev;
 }
 
 #endif /* SYS_ATOMICS_ARM_H */
diff --git a/libc/private/bionic_atomic_gcc_builtin.h b/libc/private/bionic_atomic_gcc_builtin.h
index 70eb861..2919f7f 100644
--- a/libc/private/bionic_atomic_gcc_builtin.h
+++ b/libc/private/bionic_atomic_gcc_builtin.h
@@ -16,35 +16,46 @@
 #ifndef BIONIC_ATOMIC_GCC_BUILTIN_H
 #define BIONIC_ATOMIC_GCC_BUILTIN_H
 
-/*
- * This header file is used by default if we don't have optimized atomic
+/* This header file is used by default if we don't have optimized atomic
  * routines for a given platform. See bionic_atomic_arm.h and
  * bionic_atomic_x86.h for examples.
- *
- * Note that the GCC builtins include barriers that aren't present in
- * the architecture-specific assembler versions.
  */
 
-__ATOMIC_INLINE__ void __bionic_memory_barrier() {
-  __sync_synchronize();
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier(void)
+{
+    __sync_synchronize();
+}
+
+__ATOMIC_INLINE__ int
+__bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr)
+{
+    /* We must return 0 on success */
+    return __sync_val_compare_and_swap(ptr, old_value, new_value) != old_value;
 }
 
-__ATOMIC_INLINE__ int __bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr) {
-  /* We must return 0 on success. */
-  return __sync_val_compare_and_swap(ptr, old_value, new_value) != old_value;
+__ATOMIC_INLINE__ int32_t
+__bionic_swap(int32_t new_value, volatile int32_t* ptr)
+{
+    int32_t old_value;
+    do {
+        old_value = *ptr;
+    } while (__sync_val_compare_and_swap(ptr, old_value, new_value) != old_value);
+    return old_value;
 }
 
-__ATOMIC_INLINE__ int32_t __bionic_swap(int32_t new_value, volatile int32_t* ptr) {
-  int32_t old_value;
-  do {
-    old_value = *ptr;
-  } while (__sync_val_compare_and_swap(ptr, old_value, new_value) != old_value);
-  return old_value;
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_inc(volatile int32_t* ptr)
+{
+    /* We must return the old value */
+    return __sync_fetch_and_add(ptr, 1);
 }
 
-__ATOMIC_INLINE__ int32_t __bionic_atomic_dec(volatile int32_t* ptr) {
-  /* We must return the old value. */
-  return __sync_fetch_and_add(ptr, -1);
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_dec(volatile int32_t* ptr)
+{
+    /* We must return the old value */
+    return __sync_fetch_and_add(ptr, -1);
 }
 
 #endif /* BIONIC_ATOMIC_GCC_BUILTIN_H */
diff --git a/libc/private/bionic_atomic_inline.h b/libc/private/bionic_atomic_inline.h
index f140478..6819af6 100644
--- a/libc/private/bionic_atomic_inline.h
+++ b/libc/private/bionic_atomic_inline.h
@@ -23,9 +23,14 @@
  * memory barrier needs to be issued inline rather than as a function
  * call.
  *
+ * Most code should not use these.
+ *
+ * Anything that does include this file must set ANDROID_SMP to either
+ * 0 or 1, indicating compilation for UP or SMP, respectively.
+ *
  * Macros defined in this header:
  *
- * void ANDROID_MEMBAR_FULL()
+ * void ANDROID_MEMBAR_FULL(void)
  *   Full memory barrier.  Provides a compiler reordering barrier, and
  *   on SMP systems emits an appropriate instruction.
  */
diff --git a/libc/private/bionic_atomic_mips.h b/libc/private/bionic_atomic_mips.h
index 5e08116..28fe88d 100644
--- a/libc/private/bionic_atomic_mips.h
+++ b/libc/private/bionic_atomic_mips.h
@@ -19,58 +19,84 @@
 /* Define a full memory barrier, this is only needed if we build the
  * platform for a multi-core device.
  */
-
-__ATOMIC_INLINE__ void __bionic_memory_barrier() {
 #if defined(ANDROID_SMP) && ANDROID_SMP == 1
-  __asm__ __volatile__ ( "sync" : : : "memory" );
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier()
+{
+    __asm__ __volatile__ ( "sync" : : : "memory" );
+}
 #else
-  /* A simple compiler barrier. */
-  __asm__ __volatile__ ( "" : : : "memory" );
-#endif
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier()
+{
+    /* A simple compiler barrier */
+    __asm__ __volatile__ ( "" : : : "memory" );
 }
+#endif
 
 /* Compare-and-swap, without any explicit barriers. Note that this function
  * returns 0 on success, and 1 on failure. The opposite convention is typically
  * used on other platforms.
  */
-__ATOMIC_INLINE__ int __bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr) {
-  int32_t prev, status;
-  __asm__ __volatile__ ("1: move %[status], %[new_value]  \n"
-                        "   ll %[prev], 0(%[ptr])         \n"
-                        "   bne %[old_value], %[prev], 2f \n"
-                        "   sc   %[status], 0(%[ptr])     \n"
-                        "   beqz %[status], 1b            \n"
-                        "2:                               \n"
-                        : [prev]"=&r"(prev), [status]"=&r"(status), "+m"(*ptr)
-                        : [new_value]"r"(new_value), [old_value]"r"(old_value), [ptr]"r"(ptr)
-                        : "memory");
-  return prev != old_value;
+__ATOMIC_INLINE__ int
+__bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr)
+{
+    int32_t prev, status;
+    __asm__ __volatile__ ("1: move %[status], %[new_value]  \n"
+                          "   ll %[prev], 0(%[ptr])         \n"
+                          "   bne %[old_value], %[prev], 2f \n"
+                          "   sc   %[status], 0(%[ptr])     \n"
+                          "   beqz %[status], 1b            \n"
+                          "2:                               \n"
+                          : [prev]"=&r"(prev), [status]"=&r"(status), "+m"(*ptr)
+                          : [new_value]"r"(new_value), [old_value]"r"(old_value), [ptr]"r"(ptr)
+                          : "memory");
+    return prev != old_value;
 }
 
-/* Swap, without any explicit barriers. */
-__ATOMIC_INLINE__ int32_t __bionic_swap(int32_t new_value, volatile int32_t* ptr) {
-  int32_t prev, status;
-  __asm__ __volatile__ ("1:  move %[status], %[new_value] \n"
-                        "    ll %[prev], 0(%[ptr])        \n"
-                        "    sc %[status], 0(%[ptr])      \n"
-                        "    beqz %[status], 1b           \n"
-                        : [prev]"=&r"(prev), [status]"=&r"(status), "+m"(*ptr)
-                        : [ptr]"r"(ptr), [new_value]"r"(new_value)
-                        : "memory");
-  return prev;
+
+/* Swap, without any explicit barriers */
+__ATOMIC_INLINE__ int32_t
+__bionic_swap(int32_t new_value, volatile int32_t *ptr)
+{
+   int32_t prev, status;
+    __asm__ __volatile__ ("1:  move %[status], %[new_value] \n"
+                          "    ll %[prev], 0(%[ptr])        \n"
+                          "    sc %[status], 0(%[ptr])      \n"
+                          "    beqz %[status], 1b           \n"
+                          : [prev]"=&r"(prev), [status]"=&r"(status), "+m"(*ptr)
+                          : [ptr]"r"(ptr), [new_value]"r"(new_value)
+                          : "memory");
+    return prev;
 }
 
-/* Atomic decrement, without explicit barriers. */
-__ATOMIC_INLINE__ int32_t __bionic_atomic_dec(volatile int32_t* ptr) {
-  int32_t prev, status;
-  __asm__ __volatile__ ("1:  ll %[prev], 0(%[ptr])        \n"
-                        "    addiu %[status], %[prev], -1 \n"
-                        "    sc   %[status], 0(%[ptr])    \n"
-                        "    beqz %[status], 1b           \n"
-                        : [prev]"=&r" (prev), [status]"=&r"(status), "+m" (*ptr)
-                        : [ptr]"r"(ptr)
-                        : "memory");
-  return prev;
+/* Atomic increment, without explicit barriers */
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_inc(volatile int32_t *ptr)
+{
+    int32_t prev, status;
+    __asm__ __volatile__ ("1:  ll %[prev], 0(%[ptr])        \n"
+                          "    addiu %[status], %[prev], 1  \n"
+                          "    sc   %[status], 0(%[ptr])    \n"
+                          "    beqz %[status], 1b           \n"
+                          : [prev]"=&r" (prev), [status]"=&r"(status), "+m" (*ptr)
+                          : [ptr]"r"(ptr)
+                          : "memory");
+    return prev;
 }
 
+/* Atomic decrement, without explicit barriers */
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_dec(volatile int32_t *ptr)
+{
+    int32_t prev, status;
+    __asm__ __volatile__ ("1:  ll %[prev], 0(%[ptr])        \n"
+                          "    addiu %[status], %[prev], -1 \n"
+                          "    sc   %[status], 0(%[ptr])    \n"
+                          "    beqz %[status], 1b           \n"
+                          : [prev]"=&r" (prev), [status]"=&r"(status), "+m" (*ptr)
+                          : [ptr]"r"(ptr)
+                          : "memory");
+    return prev;
+}
 #endif /* BIONIC_ATOMIC_MIPS_H */
diff --git a/libc/private/bionic_atomic_x86.h b/libc/private/bionic_atomic_x86.h
index 89639c8..aca0c4b 100644
--- a/libc/private/bionic_atomic_x86.h
+++ b/libc/private/bionic_atomic_x86.h
@@ -19,20 +19,28 @@
 /* Define a full memory barrier, this is only needed if we build the
  * platform for a multi-core device.
  */
-__ATOMIC_INLINE__ void __bionic_memory_barrier() {
 #if defined(ANDROID_SMP) && ANDROID_SMP == 1
-  __asm__ __volatile__ ( "mfence" : : : "memory" );
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier()
+{
+    __asm__ __volatile__ ( "mfence" : : : "memory" );
+}
 #else
-  /* A simple compiler barrier. */
-  __asm__ __volatile__ ( "" : : : "memory" );
-#endif
+__ATOMIC_INLINE__ void
+__bionic_memory_barrier()
+{
+    /* A simple compiler barrier */
+    __asm__ __volatile__ ( "" : : : "memory" );
 }
+#endif
 
 /* Compare-and-swap, without any explicit barriers. Note that this function
  * returns 0 on success, and 1 on failure. The opposite convention is typically
  * used on other platforms.
  */
-__ATOMIC_INLINE__ int __bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr) {
+__ATOMIC_INLINE__ int
+__bionic_cmpxchg(int32_t old_value, int32_t new_value, volatile int32_t* ptr)
+{
     int32_t prev;
     __asm__ __volatile__ ("lock; cmpxchgl %1, %2"
                           : "=a" (prev)
@@ -41,23 +49,40 @@ __ATOMIC_INLINE__ int __bionic_cmpxchg(int32_t old_value, int32_t new_value, vol
     return prev != old_value;
 }
 
-/* Swap, without any explicit barriers. */
-__ATOMIC_INLINE__ int32_t __bionic_swap(int32_t new_value, volatile int32_t *ptr) {
-  __asm__ __volatile__ ("xchgl %1, %0"
-                        : "=r" (new_value)
-                        : "m" (*ptr), "0" (new_value)
-                        : "memory");
-  return new_value;
+
+/* Swap, without any explicit barriers */
+__ATOMIC_INLINE__ int32_t
+__bionic_swap(int32_t new_value, volatile int32_t *ptr)
+{
+    __asm__ __volatile__ ("xchgl %1, %0"
+                          : "=r" (new_value)
+                          : "m" (*ptr), "0" (new_value)
+                          : "memory");
+    return new_value;
+}
+
+/* Atomic increment, without explicit barriers */
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_inc(volatile int32_t *ptr)
+{
+    int increment = 1;
+    __asm__ __volatile__ ("lock; xaddl %0, %1"
+                          : "+r" (increment), "+m" (*ptr)
+                          : : "memory");
+    /* increment now holds the old value of *ptr */
+    return increment;
 }
 
-/* Atomic decrement, without explicit barriers. */
-__ATOMIC_INLINE__ int32_t __bionic_atomic_dec(volatile int32_t* ptr) {
-  int increment = -1;
-  __asm__ __volatile__ ("lock; xaddl %0, %1"
-                        : "+r" (increment), "+m" (*ptr)
-                        : : "memory");
-  /* increment now holds the old value of *ptr */
-  return increment;
+/* Atomic decrement, without explicit barriers */
+__ATOMIC_INLINE__ int32_t
+__bionic_atomic_dec(volatile int32_t *ptr)
+{
+    int increment = -1;
+    __asm__ __volatile__ ("lock; xaddl %0, %1"
+                          : "+r" (increment), "+m" (*ptr)
+                          : : "memory");
+    /* increment now holds the old value of *ptr */
+    return increment;
 }
 
 #endif /* BIONIC_ATOMIC_X86_H */
-- 
1.8.5.2

